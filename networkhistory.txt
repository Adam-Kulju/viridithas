NET ID | general concept                          | notes
--------------------------------------------------------------------------------------------------
     0 | first network 90% eval, 10% WDL          | much weaker than the HCE.
       | 30 epochs, batch size 16384, lr 1e-2     |
       | trained on the mountain of games from    |
       | old Viridithas 2.X.X versions            |
--------------------------------------------------------------------------------------------------
     1 | second network, same data as net 0, but  | net used in v3.0.0, crushes HCE.
       | data was shuffled, which fixed problems. |
--------------------------------------------------------------------------------------------------
     2 | third network, pure WDL.                 | none
--------------------------------------------------------------------------------------------------
     3 | fourth network, pure evaluation.         | none
--------------------------------------------------------------------------------------------------
     4 | fifth network, 50/50 WDL/eval.           | none
--------------------------------------------------------------------------------------------------
  5-10 | fiddling with parameters and data        | nothing improved on net 1.
--------------------------------------------------------------------------------------------------
    11 | filtering of noisy positions, more data. | first improvement on net 1, ~20 Elo.
--------------------------------------------------------------------------------------------------
    12 | net-11 data reanalyzed with net-11.      | +50 Elo, worried about overfitting.
       |                                          | net used in v4.0.0.
--------------------------------------------------------------------------------------------------
    13 | lichess-elite games analysed with HCE,   | +20 Elo.
       | merged with the net-12 data.             |
--------------------------------------------------------------------------------------------------
    14 | net-13 data reanalyzed with net-13,      | +25 Elo.
       | deduplicated using a new tool i wrote.   | 
--------------------------------------------------------------------------------------------------
    15 | same as net-14, but trying 120 epochs,   | -41.6 +/- 7.5 Elo, LOS: 0.0 %
       | and batch size 8192.                     | vs net-14.
--------------------------------------------------------------------------------------------------
    16 | same as net-14, but trying 80 epochs,    | 111.6 +/- 18.4 Elo, LOS: 100.0 %
       | and lr drop at 30 epochs                 | vs net-14.
--------------------------------------------------------------------------------------------------
    17 | injected 320K positions from net-16      | 16.0 +/- 12.1, LOS: 99.5 %
       | into net-14 data.                        | vs net-16.
--------------------------------------------------------------------------------------------------
    18 | re-evaluated whole net-17 data with      | 23.9 +/- 7.2, LOS: 100.0 %
       | net-17.                                  | vs net-17.
--------------------------------------------------------------------------------------------------
    19 | same as net-18, but with 90% WDL focus.  | -75.3 +/- 8.0, LOS: 0.0 %
       | not intended to gain, just to test.      | vs net-18.
--------------------------------------------------------------------------------------------------
    20 | trained on 320K net-18 self-play games   | -106.2 +/- 21.2, LOS: 0.0 %
       | from the uhobook, eval'd with net-18.    | vs net-18.
--------------------------------------------------------------------------------------------------
    21 | those 320K net-18 games mixed in to the  | 7.6 +/- 6.5, LOS: 98.9 %
       | big pile of data use to train net-18.    | vs net-18.
       | NOTE/WARN: shuffled based on FEN hash.   |
--------------------------------------------------------------------------------------------------
    22 | net 21 data re-evaluated with HCE at     | -10.5 +/- 4.5, LOS: 0.0 %
       | depth 8.                                 | vs net-21.
--------------------------------------------------------------------------------------------------
    23 | net 22 data re-evaluated with net-22.    | -23.5 +/- 9.9, LOS: 0.0 %
       | Hopefully will be less overfitted.       | vs net-21.
--------------------------------------------------------------------------------------------------
    24 | net 21 data with 25% WDL focus.          | 16.1 +/- 7.6, LOS: 100.0 %
       |                                          | vs net-21.
--------------------------------------------------------------------------------------------------
    25 | 320K net-24 self-play games from uhobook | 1.0 +/- 12.2, LOS: 56.3 %
       | injected into net-24 data.               | vs net-24.
       | NOTE/WARN: shuffled based on FEN hash.   | I don't really trust this net, weird results.
--------------------------------------------------------------------------------------------------
    26 | turns out those 320K games were eval'd   | 7.1 +/- 6.5, LOS: 98.3 %
       | with HCE, so we redid it.                | vs net-24.
       | didn't pass SPRT, but it's still better. |
--------------------------------------------------------------------------------------------------
    27 | net-26 data but 40% WDL focus.           | 8.0 +/- 6.6, LOS: 99.1 %
       |                                          | vs net-26.
--------------------------------------------------------------------------------------------------
    28 | same as net-27 but with LR=5e-3.         | 2.3 +/- 6.6, LOS: 75.3 %
       |                                          | vs net-27.
--------------------------------------------------------------------------------------------------
    29 | combination of pure viri data            | ~ -60 elo vs net-28
       | from v5.1.0, v6.0.0, and v6.0.0-dev      | seems that either the Lichess Elite data has
       |                                          | really important stuff to learn, or 960k games
       |                                          | is not enough to train a good net.
--------------------------------------------------------------------------------------------------
    30 | 320K net-28 self-play games from uhobook | 7.2 +/- 6.7, LOS: 98.2 %
       | injected into net-28 data.               | vs net-28.
--------------------------------------------------------------------------------------------------
    31 | net 30 data re-evaluated with net-30.    | -3.0 +/- 6.6, LOS: 18.7 %
       | feeling somewhat discouraged.            | vs net-30.
--------------------------------------------------------------------------------------------------
    32 | net 30 data, using MinusKelvin's         | ???? +/- ????, LOS: ????
       | marlinflow fork, and his default hyper-  | vs net-30.
       | parameters:                              |
       |  - LR = 0.001                            |
       |  - LR drop at 30 epochs                  |
       |  - 45 epochs                             |
       |  - whatever his fork sets for batch size |
       |  - WDL focus 10%                         |
       | this arch uses buckets and a 384x2 net,  |
       | so it required rewriting the inference   |
       | code.                                    |
--------------------------------------------------------------------------------------------------
    33 | experiment with some Frozenight training | 12.3 +/- 6.9, LOS: 100.0 %, DrawRatio: 39.1 %
       | params while I work up the energy to     |
       | implement the new arch.                  |
       | (LR = 0.0001, 45 epochs, WDL 10%, 384N)  |
--------------------------------------------------------------------------------------------------
    34 | same as net 33, but with 512 neurons.    | -31.8 +/- 11.4, LOS: 0.0 %, DrawRatio: 42.2 %
--------------------------------------------------------------------------------------------------