NET ID | general concept                          | notes
-----------------------------------------------------------------------------------------
     0 | first network 90% eval, 10% WDL          | much weaker than the HCE.
       | 30 epochs, batch size 16384, lr 1e-2     |
       | trained on the mountain of games from    |
       | old Viridithas 2.X.X versions            |
-----------------------------------------------------------------------------------------
     1 | second network, same data as net 0, but  | net used in v3.0.0, crushes HCE.
       | data was shuffled, which fixed problems. |
-----------------------------------------------------------------------------------------
     2 | third network, pure WDL.                 | none
-----------------------------------------------------------------------------------------
     3 | fourth network, pure evaluation.         | none
-----------------------------------------------------------------------------------------
     4 | fifth network, 50/50 WDL/eval.           | none
-----------------------------------------------------------------------------------------
  5-10 | fiddling with parameters and data        | nothing improved on net 1.
-----------------------------------------------------------------------------------------
    11 | filtering of noisy positions, more data. | first improvement on net 1, ~20 Elo.
-----------------------------------------------------------------------------------------
    12 | net-11 data reanalyzed with net-11.      | +50 Elo, worried about overfitting.
       |                                          | net used in v4.0.0.
-----------------------------------------------------------------------------------------
    13 | lichess-elite games analysed with HCE,   | +20 Elo.
       | merged with the net-12 data.             |
-----------------------------------------------------------------------------------------
    14 | net-13 data reanalyzed with net-13,      | +25 Elo.
       | deduplicated using a new tool i wrote.   | 
-----------------------------------------------------------------------------------------
    15 | same as net-14, but trying 120 epochs,   | -41.6 +/- 7.5 Elo, LOS: 0.0 %
       | and batch size 8192.                     | vs net-14.
-----------------------------------------------------------------------------------------
    16 | same as net-14, but trying 80 epochs,    | 111.6 +/- 18.4 Elo, LOS: 100.0 %
       | and lr drop at 30 epochs                 | vs net-14.
-----------------------------------------------------------------------------------------
    17 | injected 320K positions from net-16      | 16.0 +/- 12.1, LOS: 99.5 %
       | into net-14 data.                        | vs net-16.
-----------------------------------------------------------------------------------------
    18 | re-evaluated whole net-17 data with      | 23.9 +/- 7.2, LOS: 100.0 %
       | net-17.                                  | vs net-17.
-----------------------------------------------------------------------------------------
    19 | same as net-18, but with 90% WDL focus.  | -75.3 +/- 8.0, LOS: 0.0 %
       | not intended to gain, just to test.      | vs net-18.
-----------------------------------------------------------------------------------------
    20 | trained on 320K net-18 self-play games   | -106.2 +/- 21.2, LOS: 0.0 %
       | from the uhobook, eval'd with net-18.    | vs net-18.
-----------------------------------------------------------------------------------------
    21 | those 320K net-18 games mixed in to the  | 7.6 +/- 6.5, LOS: 98.9 %
       | big pile of data use to train net-18.    | vs net-18.
       | NOTE/WARN: shuffled based on FEN hash.   |
-----------------------------------------------------------------------------------------
    22 | net 21 data re-evaluated with HCE at     | about -10 elo vs net 21.
       | depth 8.                                 |
-----------------------------------------------------------------------------------------
    23 | net 22 data re-evaluated with net-22.    | 
       | Hopefully will be less overfitted.       |